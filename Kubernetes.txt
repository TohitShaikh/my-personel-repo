Lauch 3 Ec2 instance with Ubuntu Server ----------------------------------------------------------------------------------

1 -Master t2.medium
2 Worker t2.micro

Connect to 3 machine -----------------------------------------------------------------------------------------------------
--------------------------------------------COMMON FOR MASTER & SLAVES(Pre-Requisite START)----------------------------------------------------
1#Switch to root user
sudo su -

2#Disabling Swap Memory
because kubernetes official documentation refers to disabliiing it 
Swap Memory - The data which is avaible n the ram and after the ram is full we swap the data of ram in the system storage
swapoff -a
sed -i '/swap/s/^\(.*\)$/#\1/' /etc/fstab


3#Install packages. To install Kubernetes and containerd run these commands
sudo apt update && sudo apt upgrade -y

sudo apt-get install -y apt-transport-https ca-certificates curl gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

#If the above command fail or show a error only that time we need to run these below two command 
sudo mkdir -p -m 755 /etc/apt/keyrings

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update -y

sudo apt-get install -y kubelet kubeadm kubectl containerd

#apt-mark will hold prevent the package from being automatically upgraded or remove
sudo apt-mark hold kubelet kubeadm kubectl containerd

4#Configure Containerd Load the necessary modules for Containerd:

cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

Enable the kernel feature
modprobe overlay
modprobe br_netfilter

5#Setup the required kernel parameters.(Enabling iptables)

cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF

sysctl --system

6#Configure containerd as default conatiner runtime

mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
systemctl restart containerd

7#Enabling kubellet service
systemctl daemon-reload
systemctl start kubelet
systemctl status kubelet

--------------------------------------------COMMON FOR MASTER & SLAVES(Pre-Requisite ENDS)-------------------------------------

In Master Node Only-------------------------------------------------------------------------------------------------------

sudo su -

#Initialize Kubernates master by executing below commond
kubeadm init

#Exit root user & exeucte as normal user

exit

kubectl version -short

#Craeting config file for kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get nodes

# To verify, if kubectl is working or not, run the following command.

kubectl get pods -n kube-system -o wide 

#You will notice from the previous command, that all the pods are running except one: ‘kube-dns’. For resolving this we will install a # pod network. 
To install the weave pod network, run the following command:

kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s-1.11.yaml

kubectl get pods --all-namespaces

# Get token

kubeadm token create --print-join-command  -----> After taking the token run the token in the worker node 

#If you are not able to join worker nodes into the control plane just edit the security group and add rule "All Tcp and give the vpc range"
Lec3----------------------------------------------------------------------------------------------------------------------

#To get info about the namespace 
kubectl get namespace

vi test-ns.yml
apiVersion: v1
kind:  Namespace
metadata: 
 name: test-ns
 labels: 
  teamname: testing

#To run the manifestfile 
kubectl apply -f test-ns.yml

kubectl get namespace


kubectl get all --all-namespaces

Lec4(PODS)----------------------------------------------------------------------------------------------------------------
vi Pods.yml 
#Template
apiVersion: v1
kind: Pod
metadata: 
 name: <podname>
 namespace: <namespace-name>
 labels: 
   <key>: <value>
spec: 
 container: 
 - name: <container-name>
   image: <imageName>
   ports: 
   - containerPort: <containerPort>

#Acual script
apiVersion: v1
kind: Pod
metadata: 
  name: maven-webapp-pod
  namespace: test-ns
  labels: 
   app: maven-webapp
spec: 
  containers: 
  - name: maven-webapp-container
    image: dockerhandson/maven-web-application:1
    ports: 
    - containerPort: 8080

kubectl apply -f Pod.yml --dry-run=client

kubectl get pods -n test-ns

kubectl describe pods maven-webapp-pod  ---> it will show error because bt default it will search the pod in the default 
                                               namespace
kubectl describe node <node-name>

kubectl describe pods maven-webapp-pod -n test-ns

kubectl delete pod  maven-webapp-pod

Lec5(Service)-------------------------------------------------------------------------------------------------------------
vi service.yml

#Template for service
apiVersion: v1
kind: Service
metadata:  
  name: <service-name>
  namespace: <ns-name>
spec: 
 type: <ClusterIP/NodePort/Loadbalncer>
 selector:                         #Pod Label as selector	
   <Pod-label-key>: <Pod-label-value>
 ports: 
 - port: <Service-Port>  #On which port the service should accept the traffic
   targetPort: <Container-Port>

#Actual script
apiVersion: v1
kind: Service
metadata: 
 name: maven-webapp-service
 namespace: test-ns
spec: 
 type: ClusterIP
 selector: 
  app: webapp
 ports: 
 - port: 80
   targetPort: 8080


kubectl apply -f service.yml 

kubectl describe svc -n test-ns

#To get the Information of labels attached to the pod

kubectl get pods --show-labels -n test-ns

#To check if the srvice which we have attached to the pod is working or not
curl -v <serviceip>:<port>/maven-web-application/

Take the ip of your service and access the pod  from the worker node 
#Create a nodejs pod 
vi nodejs.yml
apiVersion: v1
kind: Pod
metadata: 
 name: nodejs-pod
 namespace: test-ns
 labels: 
   app: nodeapp
spec: 
 containers: 
  - name: node-app-container
    image: dockerhandson/node-app-mss:1
    ports: 
      - containerPort: 9981
       
kubectl apply -f nodejs.yml

kubectl get all -n test-ns 
 
#To connect to the container inside the pod
kubectl exec -it nodejs-pod -n test-ns -- sh

#Craeting a nodeport service to access the applicatiion from outside the cluster
vi nodeport-service.yml


apiVersion: v1
kind: Service
metadata:   
 name: maven-webapp-service
 namespace: test-ns
spec: 
 type: NodePort
 selector: 
  app: maven-web-app
 ports: 
 - port: 80
   targetPort: 8080
    
kubectl apply -f nodeport-service.yml 

kubectl get all -n test-ns
output -->
NAME                   READY   STATUS    RESTARTS      AGE
pod/maven-webapp-pod   1/1     Running   1 (55m ago)   21h
pod/nodejs-pod         1/1     Running   0             20m

NAME                           TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
service/maven-webapp-service   NodePort   10.106.232.132   <none>        80:30972/TCP   21h


take the server ip and access the conatiner using the nodeport port

http://65.2.130.87:30972/maven-web-application/
Lec6(Replication-Controller)----------------------------------------------------------------------------------------------------------------
Static-Pod
vi static-pod

apiVersion: v1
kind: Pod
metadata: 
  name: static-pod
  namespace: test-ns
  labels: 
   app: static-pod
spec: 
  containers: 
  - name: maven-webapp-container
    image: dockerhandson/maven-web-application:1
    ports: 
    - containerPort: 8080

sudo cp static-pod.yml /etc/kubernetes/manifests/ -->After coping the file to this location /etc/kubernetes/manifests/ we
                                                        be able to a pod running 

sudo rm static-pod.yml in /etc/kubernetes/manifests/ -->For deleting it 

vi replication-controller.yml
#Template
apiVersion: v1
kind: ReplicationController
metadata:
  name: <ReplicationController-Name>
  namespace: <Namespace-Name>
  labels: 
    <key>: <value>
spec: 
 replicas: <No. of pod replicas>
 template: #Pod template You specify your pod details in under template section
   metadata: 
    name: <Pod-Name>
    labels: 
     <key>: <value>
   spec: 
    containers: 
    - name: <container-name>
      image: <image-name>
      ports: 
      - containerPort: <Container-Port>

#Actual Script
apiVersion: v1
kind: ReplicationController
metadata:
  name: maven-web-app-rc
  namespace: tes-ns
  labels: 
    name: rc-test
spec: 
 replicas: 2
 selector: 
   app: rc-pod
 template: #Pod template You specify your pod details in under template section
   metadata: 
    labels: 
     app: rc-pod
   spec: 
    containers: 
    - name: maven-rc-container
      image: dockerhandson/maven-web-application:1
      ports: 
      - containerPort: 8080

kubectl apply -f replication-controller.yml --dry-run=client

kubectl apply -f replication-controller.yml

Lec(Replicaset & Daemonset)--------------------------------------------------------------------------------------------
vi replicaset-demo.yml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: replicaset-demo
 namespace: test-ns
spec:
  replicas: 1
  selector:
    matchLabels:
     app: rs-demo
  template:
    metadata:
     labels:
      app: rs-demo
    spec:
      containers:
      - name: replicaset-demo
        image: dockerhandson/node-app-mss:1
        ports:
         - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
 name: replicaset-demo
 namespace: test-ns
spec:
 type:NodePort
 selector:
  app: rs-demo
 ports:
 - port: 80
   targetPort: 9981

kubectl apply -f replicaset-demo.yml --dry-run=server

kubectl apply -f replicaset-demo.yml

#For forcefully deleting the resources
kubectl delete rs replicaset-demo -n test-ns --force --grace-period=0

vi daemonset-demo.yml
apiVersion: apps/v1
kind: DaemonSet
metadata: 
 name: daemonset-demo
 namespace: test-ns
spec: 
 selector: 
  matchLabels: 
   app: demo
 template: 
  metadata: 
   labels: 
    app: demo
  spec: 
   containers: 
   - name: daemonset-demo-container
     image: nginx
     ports: 
     - containerPort: 80