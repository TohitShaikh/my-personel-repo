Lauch 4 Ec2 instance with Ubuntu Server ----------------------------------------------------------------------------------

1 Master t2.medium
3 Worker t2.micro

Connect to 3 machine -----------------------------------------------------------------------------------------------------
--------------------------------------------COMMON FOR MASTER & SLAVES(Pre-Requisite START)----------------------------------------------------
1#Switch to root user
sudo su -

2#Disabling Swap Memory
because kubernetes official documentation refers to disabliiing it 
Swap Memory - The data which is avaible n the ram and after the ram is full we swap the data of ram in the system storage
swapoff -a
sed -i '/swap/s/^\(.*\)$/#\1/' /etc/fstab


3#Install packages. To install Kubernetes and containerd run these commands
sudo apt update && sudo apt upgrade -y

sudo apt-get install -y apt-transport-https ca-certificates curl gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

#If the above command fail or show a error only that time we need to run these below two command 
sudo mkdir -p -m 755 /etc/apt/keyrings

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update -y

sudo apt-get install -y kubelet kubeadm kubectl containerd

#apt-mark will hold prevent the package from being automatically upgraded or remove
sudo apt-mark hold kubelet kubeadm kubectl containerd

4#Configure Containerd Load the necessary modules for Containerd:

cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

Enable the kernel feature
modprobe overlay
modprobe br_netfilter

5#Setup the required kernel parameters.(Enabling iptables)

cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF

sysctl --system

6#Configure containerd as default conatiner runtime

mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
systemctl restart containerd

7#Enabling kubellet service
systemctl daemon-reload
systemctl start kubelet
systemctl status kubelet

--------------------------------------------COMMON FOR MASTER & SLAVES(Pre-Requisite ENDS)-------------------------------------

In Master Node Only-------------------------------------------------------------------------------------------------------

sudo su -

#Initialize Kubernates master by executing below commond
kubeadm init

#Exit root user & exeucte as normal user

exit

kubectl version -short

#Craeting config file for kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get nodes

# To verify, if kubectl is working or not, run the following command.

kubectl get pods -n kube-system -o wide 

#You will notice from the previous command, that all the pods are running except one: ‘kube-dns’. For resolving this we will install a # pod network. 
To install the weave pod network, run the following command:

kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s-1.11.yaml

kubectl get pods --all-namespaces

# Get token

kubeadm token create --print-join-command  -----> After taking the token run the token in the worker node 

#If you are not able to join worker nodes into the control plane just edit the security group and add rule "All Tcp and give the vpc range"
Lec3----------------------------------------------------------------------------------------------------------------------

#To get info about the namespace 
kubectl get namespace

vi test-ns.yml
apiVersion: v1
kind:  Namespace
metadata: 
 name: test-ns
 labels: 
  teamname: testing

#To run the manifestfile 
kubectl apply -f test-ns.yml

kubectl get namespace


kubectl get all --all-namespaces

Lec4(PODS)----------------------------------------------------------------------------------------------------------------
vi Pods.yml 
#Template
apiVersion: v1
kind: Pod
metadata: 
 name: <podname>
 namespace: <namespace-name>
 labels: 
   <key>: <value>
spec: 
 container: 
 - name: <container-name>
   image: <imageName>
   ports: 
   - containerPort: <containerPort>

#Acual script
apiVersion: v1
kind: Pod
metadata: 
  name: maven-webapp-pod
  namespace: test-ns
  labels: 
   app: maven-webapp
spec: 
  containers: 
  - name: maven-webapp-container
    image: dockerhandson/maven-web-application:1
    ports: 
    - containerPort: 8080

kubectl apply -f Pod.yml --dry-run=client

kubectl get pods -n test-ns

kubectl describe pods maven-webapp-pod  ---> it will show error because bt default it will search the pod in the default 
                                               namespace
kubectl describe node <node-name>

kubectl describe pods maven-webapp-pod -n test-ns

kubectl delete pod  maven-webapp-pod

Lec5(Service)-------------------------------------------------------------------------------------------------------------
vi service.yml

#Template for service
apiVersion: v1
kind: Service
metadata:  
  name: <service-name>
  namespace: <ns-name>
spec: 
 type: <ClusterIP/NodePort/Loadbalncer>
 selector:                         #Pod Label as selector	
   <Pod-label-key>: <Pod-label-value>
 ports: 
 - port: <Service-Port>  #On which port the service should accept the traffic
   targetPort: <Container-Port>

#Actual script
apiVersion: v1
kind: Service
metadata: 
 name: maven-webapp-service
 namespace: test-ns
spec: 
 type: ClusterIP
 selector: 
  app: webapp
 ports: 
 - port: 80
   targetPort: 8080


kubectl apply -f service.yml 

kubectl describe svc -n test-ns

#To get the Information of labels attached to the pod

kubectl get pods --show-labels -n test-ns

#To check if the srvice which we have attached to the pod is working or not
curl -v <serviceip>:<port>/maven-web-application/

Take the ip of your service and access the pod  from the worker node 
#Create a nodejs pod 
vi nodejs.yml
apiVersion: v1
kind: Pod
metadata: 
 name: nodejs-pod
 namespace: test-ns
 labels: 
   app: nodeapp
spec: 
 containers: 
  - name: node-app-container
    image: dockerhandson/node-app-mss:1
    ports: 
      - containerPort: 9981
       
kubectl apply -f nodejs.yml

kubectl get all -n test-ns 
 
#To connect to the container inside the pod
kubectl exec -it nodejs-pod -n test-ns -- sh

#Craeting a nodeport service to access the applicatiion from outside the cluster
vi nodeport-service.yml


apiVersion: v1
kind: Service
metadata:   
 name: maven-webapp-service
 namespace: test-ns
spec: 
 type: NodePort
 selector: 
  app: maven-web-app
 ports: 
 - port: 80
   targetPort: 8080
    
kubectl apply -f nodeport-service.yml 

kubectl get all -n test-ns
output -->
NAME                   READY   STATUS    RESTARTS      AGE
pod/maven-webapp-pod   1/1     Running   1 (55m ago)   21h
pod/nodejs-pod         1/1     Running   0             20m

NAME                           TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
service/maven-webapp-service   NodePort   10.106.232.132   <none>        80:30972/TCP   21h


take the server ip and access the conatiner using the nodeport port

http://100.27.187.183:32676/maven-web-application/
Lec6(Replication-Controller)----------------------------------------------------------------------------------------------------------------
Static-Pod
vi static-pod

apiVersion: v1
kind: Pod
metadata: 
  name: static-pod
  namespace: test-ns
  labels: 
   app: static-pod
spec: 
  containers: 
  - name: maven-webapp-container
    image: tohitshaikh/jenkins-image
    ports: 
    - containerPort: 8080

sudo cp static-pod.yml /etc/kubernetes/manifests/ -->After coping the file to this location /etc/kubernetes/manifests/ we
                                                        be able to a pod running 

sudo rm static-pod.yml in /etc/kubernetes/manifests/ -->For deleting it 

vi replication-controller.yml
#Template
apiVersion: v1
kind: ReplicationController
metadata:
  name: <ReplicationController-Name>
  namespace: <Namespace-Name>
  labels: 
    <key>: <value>
spec: 
 replicas: <No. of pod replicas>
 template: #Pod template You specify your pod details in under template section
   metadata: 
    name: <Pod-Name>
    labels: 
     <key>: <value>
   spec: 
    containers: 
    - name: <container-name>
      image: <image-name>
      ports: 
      - containerPort: <Container-Port>

#Actual Script
apiVersion: v1
kind: ReplicationController
metadata:
  name: maven-web-app-rc
  namespace: tes-ns
  labels: 
    name: rc-test
spec: 
 replicas: 2
 selector: 
   app: rc-pod
 template: #Pod template You specify your pod details in under template section
   metadata: 
    labels: 
     app: rc-pod
   spec: 
    containers: 
    - name: maven-rc-container
      image: dockerhandson/maven-web-application:1
      ports: 
      - containerPort: 8080

kubectl apply -f replication-controller.yml --dry-run=client

kubectl apply -f replication-controller.yml

Lec7(Replicaset & Daemonset)--------------------------------------------------------------------------------------------
vi replicaset-demo.yml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: replicaset-demo
 namespace: test-ns
spec:
  replicas: 1
  selector:
    matchLabels:
     app: rs-demo
  template:
    metadata:
     labels:
      app: rs-demo
    spec:
      containers:
      - name: replicaset-demo
        image: dockerhandson/node-app-mss:1
        ports:
         - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
 name: replicaset-demo
 namespace: test-ns
spec:
 type:NodePort
 selector:
  app: rs-demo
 ports:
 - port: 80
   targetPort: 9981

kubectl apply -f replicaset-demo.yml --dry-run=server

kubectl apply -f replicaset-demo.yml

#For forcefully deleting the resources
kubectl delete rs replicaset-demo -n test-ns --force --grace-period=0

vi daemonset-demo.yml
apiVersion: apps/v1
kind: DaemonSet
metadata: 
 name: daemonset-demo
 namespace: test-ns
spec: 
 selector: 
  matchLabels: 
   app: demo
 template: 
  metadata: 
   labels: 
    app: demo
  spec: 
   containers: 
   - name: daemonset-demo-container
     image: nginx
     ports: 
     - containerPort: 80

Lec8(Deployment & Deployment strategies)-------------------------------------------------------------------------------------------------------

vi deployment-recreate.yml

apiVersion: apps/v1
kind: Deployment
metadata: 
 name: demo-deployment
 namespace: test-ns
 labels: 
  app: demo
spec: 
 replicas: 2
 strategy: 
  type: Recreate
 selector: 
  matchLables: 
   app: demo
 template: 
  metadata: 
   name: demo-pod
   labels: 
    app: demo
  spec: 
   containers: 
   - name: demo-container
     image: dockerhandson/maven-web-application:2
     ports: 
     - containerPort: 8080

kubectl apply -f deployment.yml --dry-run=client

kubectl apply -f deployment.yml

#To check the status of deployment as if ti is deployed successfully
kubectl rollout status deployment <deployment_name> -n <namespace_name>

#To check history of deployment / to check which version is deployed
kubectl rollout history deployment <deployment_name> -n <namespace_name>

#To check detailed history of the revision/histroy
kubectl rollout history deployment <deployment_name> -n test-ns --revision <revsion_number>

#To go back to previous version or rollback to previous version
kubectl rollout undo deployment demo-deployment -n test-ns

#PollingUpdate Practical

vi deployment-rollingupdate.yml

apiVersion: apps/v1
kind: Deployment
metadata:
 name: demo-deployment
 namespace: test-ns
spec:
 replicas: 2
 revisionHistoryLimit: 5
 strategy:
  type: RollingUpdate
  rollingUpdate:
   maxUnavailable: 1
   maxSurge: 1
 minReadySeconds: 30
 selector:
  matchLabels:
   app: demo
 template:
  metadata:
   name: demo-deployment-pod
   labels:
    app: demo
  spec:
   containers:
   - name: demo-container
     image: dockerhandson/maven-web-application:15
     ports:
     - containerPort: 8080

kubectl apply -f deployment-rollingupdate.yml

#To see the changes in deployment or to see if the rollingUpdate is working properly or not in the cluster we need to update(change the image tag to 20 and reapply it) manifest file to see the changes
kubectl apply -f deployment-rollingupdate.yml --record=true


#After running above command see the change in the cluster new pod will be created Change
kubectl rollout history deployment demo-deployment -n test-ns

Lec5(Pod Autoscaler)-------------------------------------------------------------------------------------------------------------------------------------------------

#Rather downloading it from website we will bring that file in local laptop and edit it accordingly and share it to server and then use it 

#We are intalling metric server but before installation we need to change some configuration so that it can run.

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

#Check if the metric server is running or not
kubectl top pod -A

#Deploy a sample application with deployment

vi hpadeployment

apiVersion: apps/v1
kind: Deployment
metadata:
 name: hpadeployment
 namespace: test-ns
 labels:
  name: hpapod
spec:
 replicas: 2
 selector:
  matchLabels:
   name: hpapod
 template:
  metadata:
   labels:
    name: hpapod
  spec:
   containers:
    - name: hpacontainer
      image: k8s.gcr.io/hpa-example
      ports:
      - name: http
        containerPort: 80
      resources:
        requests:
         cpu: "100m"
         memory: "64Mi"  # Corrected here
        limits:
         cpu: "100m"
         memory: "256Mi"  # Corrected here
---
apiVersion: v1
kind: Service
metadata:
 name: hpaclusterservice
 namespace: test-ns
 labels:
  name: hpaservice
spec:
 ports:
  - port: 80
    targetPort: 80
 selector:
  name: hpapod
 type: NodePort

#We will create a pod with busybox container to test if our load is getting distributed or not or to watch utilization of resources on node

kubectl run load-generator -i --tty --rm --image=busybox -n test-ns -- /bin/sh

#After connecting to the container we will check if th application is reponding
wget -qO- http://hpaclusterservice

#We will put load on the conatainer to observe the resources utilization 
while true; do wget -q -O- http://hpaclusterservice; done

#We will create a kubernetes object/resource(HPA) for the deployment to horizontally scale the the pod on observe resorce utilization

vi pod-autoscaler.yml

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata: 
 name: hpa-autoscaler
 namespace: test-ns
spec: 
 scaleTargetRef: 
  apiVersion: apps/v1
  kind: Deployment
  name: hpadeployment
 minReplicas: 2
 maxReplicas: 5
 metrics: 
 - type: Resources
   resource: 
    name: cpu
    target: 
     type: Utilization
     averageUtilization: 40
 - type: Resources
   resource: 
    name: memory
    target: 
     type: Utilization
     averageUtilization: 40

#After creating the pod autoscaler we will create a temprary pod to test if the pod auto scaler is working or not and run a query

kubectl run load-generator -i --tty --rm --image=busybox -n test-ns -- /bin/sh

while true; do wget -q -O- http://hpaclusterservice; done

Lec11(Volumes)---------------------------------------------------------------------------------------------------------------------------------------------------------

#We are creating a deployement with a database attached

vi spring.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springapp
  namespace: test-ns  # You can change this to your desired namespace
  labels:
    app: springapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo:1  #This application will communicate with another application like database so we will pass arguments (host, passwords.)
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "500Mi"
            cpu: "500m"
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongosvc
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
---
apiVersion: v1 #We need a service to access above application
kind: Service
metadata:
  name: springappsvc
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---       
#We are craeting replicaset for database 

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodb
  namespace: test-ns
  labels:
    app: mongodb
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongocontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
---
apiVersion: v1
kind: Service
metadata:
  name: mongosvc
  namespace: test-ns
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort:  27017

kubectl apply -f spring.yml 

#As we have configured 1 replica in our mongo database replicaset so for any reason it goes then there is data loss

#For our existing script of database and web application we will add volumes to database

vi sprong-volume.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springapp
  namespace: test-ns  # You can change this to your desired namespace
  labels:
    app: springapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo:1  #This application will communicate with another application like database so we will pass arguments (host, passwords.)
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "500Mi"
            cpu: "500m"
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongosvc
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
---
apiVersion: v1 #We need a service to access above application
kind: Service
metadata:
  name: springappsvc
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodb
  namespace: test-ns
  labels:
    app: mongodb
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongocontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts: 
        - name: mongodb-host-volume
          mountPath: /data/db
      volumes: 
      - name: mongodb-host-volume
        hostPath: 
         path: /mongodata
---
apiVersion: v1
kind: Service
metadata:
  name: mongosvc
  namespace: test-ns
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort:  27017

kubectl apply -f spring-volume.yml

#After applying the volume we will check of the volume is attached successfully or not
kubectl describe pod mongodb-7sdzw -n test-ns

#Connect to the machine which the pod is schedule and observe that the there directory will be created

#We will create nfs server  on our ec2 intance so it will act as a database 

sudo apt update && sudo apt upgrade -y

sudo apt install nfs-kernel-server -y

sudo mkdir -p /mnt/nfs_share -->This is the share directory which will be used to share with client machine

sudo chmod 777 /mnt/nfs_share --> We will change permission becauese it  will accessible by remote client machine

sudo chown -R nobody:nogroup /mnt/nfs_share

sudo vi /mnt/nfs_share -->Open this file and add the below data because the file will be accessed by remote user/machine

/mnt/nfs_share *(rw,sync,no_subtree_check,no_root_squash)

#After saving the above file run below command

sudo exportfs -a
sudo systemctl restart nfs-kernel-server

#We will install nfs on all the nodes in the cluster
sudo apt install nfs-common -y

#To use nfs sever in the pod we need to specify the nfs configuration in the pod manifest

vi spring-nfs-volume.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springapp
  namespace: test-ns  # You can change this to your desired namespace
  labels:
    app: springapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo:1  #This application will communicate with another application like database so we will pass arguments (host, passwords.)
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "500Mi"
            cpu: "500m"
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongosvc
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
---
apiVersion: v1 #We need a service to access above application
kind: Service
metadata:
  name: springappsvc
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodb
  namespace: test-ns
  labels:
    app: mongodb
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongocontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts: 
        - name: mongodb-host-volume
          mountPath: /data/db
      volumes: 
      - name: mongodb-host-volume
        nfs: 
         server: 172.31.17.82 
         path: /mnt/nfs_share
---
apiVersion: v1
kind: Service
metadata:
  name: mongosvc
  namespace: test-ns
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort:  27017

kubectl apply -f spring-nfs-volume.yml

#After apply the above file we will observer in the nfs_server there files will be created

Lec12(PersistentVolume && PersistentVolumeClaim)-----------------------------------------------------------------------------------------------------------------------

#For understanding and first time we will purposly create every resources step by step for understanding purpose

vi spring-pvc.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springapp
  namespace: test-ns  # You can change this to your desired namespace
  labels:
    app: springapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo:1  #This application will communicate with another application like database so we will pass arguments (host, passwords.)
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "500Mi"
            cpu: "500m"
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongosvc
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
---
apiVersion: v1 #We need a service to access above application
kind: Service
metadata:
  name: springappsvc
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodb
  namespace: test-ns
  labels:
    app: mongodb
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongocontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts: 
        - name: mongodb-host-volume
          mountPath: /data/db
      volumes: 
      - name: mongodb-host-volume
        persistentVolumeClaim: 
         claimName: mongopvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
 name: mongopvc
 namespace: test-ns
spec: 
 resources: 
  request: 
   storage: 1Gi
 accessModes: 
 - ReadWriteOnce
---
apiVersion: v1
kind: Service
metadata:
  name: mongosvc
  namespace: test-ns
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort:  27017

kubectl apply -f spring-pvc.yml

kubeclt describe pvc spring-pvc.yml -n test-ns

vi nfs_pv.yml

apiVersion: v1
kind: PersistentVolume
metadata: 
 name: nfs-pv
 namespace: test-ns
spec: 
 capacity: 
   storage: 1Gi
 accessModes: 
 - ReadWriteMany
 nfs: 
  server: 172.31.17.82 
  path: /mnt/nfs_share

kubectl apply -f nfs_pv.yml

kubectl get pv -n test-ns

#After creating the PV it will not be associated with any PVC because of diffrent accessmode 

#We will create a hostpath pv and will associate it with pvc for just demo purpose

vi hostpath_pv.yml

apiVersion: v1
kind: PersistentVolume
metadata: 
 name: hostpath-pv
 namespace: test-ns
spec: 
 capacity: 
   storage: 2Gi
 accessModes: 
 - ReadWriteOnce
 hostPath
  path: /mongo-db-data

kubectl apply -f hostpath_pv.yml
 	
kubectl get pv

Lec13(Storage Class & ConfigMaps & Secret)-----------------------------------------------------------------------------------------------------------------------

#We will use nfs server as provisioner for storage class so it create nfs volume

#This below nfs provsioner will use existing nfs server to provsion storage class
#Update the private ip address for nfs server
Install nfs provisoner for kubernetes

vi nfs-storageclass.yml

---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: nfs-pod-provisioner-sa
  namespace: kube-system
---
kind: ClusterRole # Role of kubernetes
apiVersion: rbac.authorization.k8s.io/v1 # auth API
metadata:
  name: nfs-provisioner-clusterRole
rules:
  - apiGroups: [""] # rules on persistentvolumes
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-provisioner-rolebinding
subjects:
  - kind: ServiceAccount
    name: nfs-pod-provisioner-sa # defined on top of file
    namespace: kube-system
roleRef: # binding cluster role to service account
  kind: ClusterRole
  name: nfs-provisioner-clusterRole # name defined in clusterRole
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-pod-provisioner-otherRoles
  namespace: kube-system
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-pod-provisioner-otherRoles
  namespace: kube-system
subjects:
  - kind: ServiceAccount
    name: nfs-pod-provisioner-sa # same as top of the file
    # replace with namespace where provisioner is deployed
    namespace: kube-system
roleRef:
  kind: Role
  name: nfs-pod-provisioner-otherRoles
  apiGroup: rbac.authorization.k8s.io
  
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-pod-provisioner
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-pod-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-pod-provisioner
    spec:
      serviceAccountName: nfs-pod-provisioner-sa # name of service account created in rbac.yaml
      containers:
        - name: nfs-pod-provisioner
          image: rkevin/nfs-subdir-external-provisioner:fix-k8s-1.20
          volumeMounts:
            - name: nfs-provisioner-v
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME # do not change
              value: nfs-provisioner # SAME AS PROVISONER NAME VALUE IN STORAGECLASS
            - name: NFS_SERVER # do not change
              value: 172.31.17.82 # Ip of the NFS SERVER
            - name: NFS_PATH # do not change
              value: /mnt/nfs_share # path to nfs directory setup
      volumes:
       - name: nfs-provisioner-v # same as volumemouts name
         nfs:
           server: 172.31.17.82
           path:  /mnt/nfs_share   
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storageclass # IMPORTANT pvc needs to mention this name
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: nfs-provisioner # name can be anything
parameters:
  archiveOnDelete: "false"

kubectl apply -f nfs_StrorageClass.yml

#After applying the storage class you will see a provsioner in kube-system namespace

kubectl get all -n kube-system

kubectl get sc
NAME                         PROVISIONER       RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-storageclass (default)   nfs-provisioner   Delete          Immediate           false                  3m9s
#we cam customize the reclaimpolicy to our desired polcy

#After configuring the storage class successfully we will deploy this file "spring_pvc.yml" and observer the pv
kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS       VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-013ea502-ebbd-49d1-88c7-23ff0c390573   1Gi        RWO            Delete           Bound    test-ns/mongopvc   nfs-storageclass   <unset>                          

This above pv is managed by storage class 

#We will see a subfolder in the nfs server

#We will create a sample pod for demo using strage class

vi jenkins-pvc.yml

apiVersion: v1
kind: Namespace
metadata: 
  name: devops-ns
  labels:
    team: devopc
    tools: cicdtools
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jenkinsdeployment
  namespace: devops-ns
spec:
  selector:
    matchLabels:
      app: jenkins
  template:
    metadata:
      labels:
        app: jenkins
    spec:
      containers:
      - name: jenkinscontainer
        image: jenkins/jenkins:lts
        resources:
          requests:
             cpu: 200m
             memory: 256Mi
          limits:
            memory: "512Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: jenkinsvol
          mountPath: /var/jenkins_home
      volumes:
      - name: jenkinsvol
        persistentVolumeClaim:
          claimName: jenkinspvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jenkinspvc
  namespace: devops-ns
spec:
  resources:
    requests:
      storage: 2Gi
  accessModes:
    - ReadWriteOnce
---
apiVersion: v1
kind: Service
metadata:
  name: jenkinssvc
  namespace: devops-ns
spec:
  type: NodePort
  selector:
    app: jenkins
  ports:
  - port: 80
    targetPort: 8080

kubectl apply -f jenkins-pvc.yml

#ConfigMap----------------------------------------------------------------------------------------------------------------------

vi config-map.yml

apiVersion: v1
kind: ConfigMap
metadata: 
 name: springappconfigmap
 namespace: test-ns
data: 
 mongodbusername: devdb
 mongodbpassword: devdb@123

kubectl apply -f configMap.yml

vi spring-app-configmap.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springapp
  namespace: test-ns  # You can change this to your desired namespace
  labels:
    app: springapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo:1
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "500Mi"
            cpu: "500m"
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongosvc
        - name: MONGO_DB_USERNAME
          valueFrom: 
           configMapKeyRef: 
            name: springappconfigmap
            key: mongodbusername
        - name: MONGO_DB_PASSWORD
          valueFrom: 
           configMapKeyRef: 
            name: springappconfigmap
            key: mongodbpassword
---
apiVersion: v1 #We need a service to access above application
kind: Service
metadata:
  name: springappsvc
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodb
  namespace: test-ns
  labels:
    app: mongodb
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongocontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom: 
           configMapKeyRef: 
            name: springappconfigmap
            key: mongodbusername
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom: 
           configMapKeyRef: 
            name: springappconfigmap
            key: mongodbpassword
        volumeMounts: 
        - name: mongodb-host-volume
          mountPath: /data/db
      volumes: 
      - name: mongodb-host-volume
        persistentVolumeClaim: 
         claimName: mongopvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
 name: mongopvc
 namespace: test-ns
spec: 
 accessModes: 
 - ReadWriteOnce
 resources: 
  requests: 
   storage: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: mongosvc
  namespace: test-ns
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort:  27017


kubectl apply -f spring-app-configMap.yml

kubectl get all -n test-ns

#It will show error in container status

kubectl get all -n test-ns
NAME                             READY   STATUS                       RESTARTS   AGE
pod/mongodb-kj2q7                0/1     CreateContainerConfigError   0          2m12s
pod/springapp-858b9dc79b-lhhsw   0/1     CreateContainerConfigError   0          2m12s
pod/springapp-858b9dc79b-wrkkd   0/1     CreateContainerConfigError   0          2m12s

kubectl describe pod mongodb-kj2q7 -n test-ns

#Run the configMap script to get the pod up and runnning

kubectl describe cm springappconfigmap -n test-ns
Name:         springappconfigmap
Namespace:    test-ns
Labels:       <none>
Annotations:  <none>

Data
====
mongodbpassword:
----
devdb@123
mongodbusername:
----
devdb

BinaryData
====

Events:  <none>

Secret

vi config-secret.yml

apiVersion: v1
kind: ConfigMap
metadata: 
 name: springappconfigmap
 namespace: test-ns
data: 
 mongodbusername: devdb
---
apiVersion: v1
kind: Secret
metadata: 
 name: springappsecret
 namespace: test-ns
type: Opaque
stringData: 
 mongodbpassword: devdb@123

vi configMap-Secret-springapp.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springapp
  namespace: test-ns  # You can change this to your desired namespace
  labels:
    app: springapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: dockerhandson/spring-boot-mongo:1
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "500Mi"
            cpu: "500m"
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongosvc
        - name: MONGO_DB_USERNAME
          valueFrom: 
           configMapKeyRef: 
            name: springappconfigmap
            key: mongodbusername
        - name: MONGO_DB_PASSWORD
          valueFrom: 
           secretKeyRef: 
            name: springappsecret
            key: mongodbpassword
---
apiVersion: v1 #We need a service to access above application
kind: Service
metadata:
  name: springappsvc
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodb
  namespace: test-ns
  labels:
    app: mongodb
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongocontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom: 
           configMapKeyRef: 
            name: springappconfigmap
            key: mongodbusername
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom: 
           secretKeyRef: 
            name: springappsecret
            key: mongodbpassword
        volumeMounts: 
        - name: mongodb-host-volume
          mountPath: /data/db
      volumes: 
      - name: mongodb-host-volume
        persistentVolumeClaim: 
         claimName: mongopvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
 name: mongopvc
 namespace: test-ns
spec: 
 accessModes: 
 - ReadWriteOnce
 resources: 
  requests: 
   storage: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: mongosvc
  namespace: test-ns
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort:  27017

kubectl apply -f configMap-Secret-springapp.yml 
#After running above file it will the username from configMap and password from secret

Lec14(Private-Repository & Liveness & Readyness)----------------------------------------------------------------------------------------------------------------------

#Pulling image from private repository

vi private-repo-demo.yml

apiVersion: apps/v1
kind: Deployment
metadata: 
 name: private-repo-demo
 namespace: test-ns
 labels: 
  app: demo
spec: 
 replicas: 1
 selector: 
  matchLabels: 
   app: demo
 template: 
  metadata: 
   name: demo-pod
   labels: 
    app: demo
  spec: 
   containers: 
   - name: demo-container
     image: tohitshaikh/jenkins-image:latest
     ports: 
     - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
 name: private-repo-svc
 namespace: test-ns
spec:
 type: NodePort
 selector:
  name: demo-pod
 ports:
 - port: 80
   targetPort: 8080

kubectl apply -f private-repo-demo.yml

#After appplying the file it will show error in the the pod status 

#We will create a secret of docker type and refer it in manifest

#Create a Secret by providing credentials on the command line

kubectl create secret docker-registry dockerhubcred --docker-server=https://index.docker.io/v1/ --docker-username=tohitshaikh --docker-password=Tohit8390# --docker-email=test@gmail.com --namespace=test-ns

#Will update in the file with imagepullsecret
vi private-repo-demo.yml

apiVersion: apps/v1
kind: Deployment
metadata: 
 name: private-repo-demo
 namespace: test-ns
 labels: 
  app: demo
spec: 
 replicas: 1
 selector: 
  matchLabels: 
   app: demo
 template: 
  metadata: 
   name: demo-pod
   labels: 
    app: demo
  spec: 
   containers: 
   - name: demo-container
     image: tohitshaikh/jenkins-image:latest
     ports: 
     - containerPort: 8080
   imagePullSecrets: 
   - name: dockerhubcred
---
apiVersion: v1
kind: Service
metadata:
 name: private-repo-svc
 namespace: test-ns
spec:
 type: NodePort
 selector:
  name: demo-pod
 ports:
 - port: 80
   targetPort: 8080

Liveness & Readyness ------------------------------------------------------------------------------------------------------------------------------------------------

#As we will not able to create deadlock situation or momory leaakgae 

vi test-application.yml

apiVersion: apps/v1
kind: Deployment
metadata: 
 name: test-app
 namespace: test-ns
 labels: 
  app: test
spec: 
 replicas: 2
 selector: 
  matchLabels: 
   app: test
 template: 
  metadata: 
   name: demo-pod
   labels: 
    app: test
  spec: 
   containers: 
   - name: demo-container
     image: dockerhandson/mavenwebapplication:1
     ports: 
     - containerPort: 808
---
apiVersion: v1
kind: Service
metadata:
 name: test-svc
 namespace: test-ns
spec:
 type: NodePort
 selector:
  app: test
 ports:
 - port: 80
   targetPort: 8080

kubectl apply -f test-application.yml

kubectl get all -n test-ns

#We will check the application and we will be receiving traffic from both the pod
curl -v 10.98.166.17/maven-web-application/

#As we will not able to create deadlock situation or momory leakage we will intentionally delete war file from tje pod/container

kubectl exec test-app-6946659b4f-rfrcl -n test-ns -- ls /usr/local/tomcat/webapps

kubectl exec test-app-6946659b4f-n4wcm -n test-ns -- rm /usr/local/tomcat/webapps/maven-web-application.war
